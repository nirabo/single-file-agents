# Configuration for sfa_ollama_delegator_agent.py
# Copy this file to .env and fill in your actual values.
# Lines starting with # are comments.

# --- Primary LLM Provider ---
# Choose your primary LLM provider. 
# Currently supported: "ollama", "anthropic". 
# The agent is designed to be extensible for other providers in the future.
PRIMARY_LLM_PROVIDER="ollama"
# Example for Anthropic:
# PRIMARY_LLM_PROVIDER="anthropic" 

# --- Ollama Configuration (used if PRIMARY_LLAMA_PROVIDER is "ollama" or for the ollama_generate tool) ---
OLLAMA_BASE_URL="http://localhost:11434"

# Model to use if Ollama is the primary LLM provider
OLLAMA_PRIMARY_MODEL_NAME="gemma2:9b" 
# Examples: "llama3:8b", "mistral:7b", "gemma2:9b"

# Default model for the 'run_ollama_generate' tool if not specified by the primary LLM.
# The primary LLM can still choose to use any model for the tool.
OLLAMA_DEFAULT_DELEGATE_MODEL_NAME="gemma2:2b"
# Examples: "llama2:7b", "codellama:7b", "gemma2:2b"


# --- Anthropic Configuration (used if PRIMARY_LLM_PROVIDER is "anthropic") ---
# ANTHROPIC_API_KEY="sk-ant-your-api-key-here"

# Default model if Anthropic is the primary LLM provider
ANTHROPIC_MODEL_NAME="claude-3-haiku-20240307"
# Examples: "claude-3-opus-20240229", "claude-3-sonnet-20240229", "claude-3-haiku-20240307"


# --- Agent Behavior ---
# Maximum number of agent loops (CLI arg --compute overrides this)
# DEFAULT_MAX_COMPUTE_LOOPS="7" 

# Note: CLI arguments will override settings in this .env file.
